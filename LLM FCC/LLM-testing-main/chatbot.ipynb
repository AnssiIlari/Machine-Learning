{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tJkWKvWsHk4ZTjUE3EEmoX9dme7I28lp","timestamp":1703238872453}],"gpuType":"T4","mount_file_id":"1vmUEFMUhwsHIX2m9lUUgA-iiEym2HFm9","authorship_tag":"ABX9TyPJD3oqJxYWOb9Qii4y0m96"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"olKa_2DH504o","executionInfo":{"status":"error","timestamp":1703242172839,"user_tz":-120,"elapsed":59528,"user":{"displayName":"Anssi Laitinen","userId":"06177017075906697482"}},"outputId":"79e216c8-a68f-4d4c-8e4b-79a961ac124a"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","loaded successfully!\n","Prompt:\n","Hello!\n","torch.Size([1, 6])\n","torch.Size([1, 7])\n","torch.Size([1, 8])\n","torch.Size([1, 9])\n","torch.Size([1, 10])\n","torch.Size([1, 11])\n","torch.Size([1, 12])\n","torch.Size([1, 13])\n","torch.Size([1, 14])\n","torch.Size([1, 15])\n","torch.Size([1, 16])\n","torch.Size([1, 17])\n","torch.Size([1, 18])\n","torch.Size([1, 19])\n","torch.Size([1, 20])\n","torch.Size([1, 21])\n","torch.Size([1, 22])\n","torch.Size([1, 23])\n","torch.Size([1, 24])\n","torch.Size([1, 25])\n","torch.Size([1, 26])\n","torch.Size([1, 27])\n","torch.Size([1, 28])\n","torch.Size([1, 29])\n","torch.Size([1, 30])\n","torch.Size([1, 31])\n","torch.Size([1, 32])\n","torch.Size([1, 33])\n","torch.Size([1, 34])\n","torch.Size([1, 35])\n","torch.Size([1, 36])\n","torch.Size([1, 37])\n","torch.Size([1, 38])\n","torch.Size([1, 39])\n","torch.Size([1, 40])\n","torch.Size([1, 41])\n","torch.Size([1, 42])\n","torch.Size([1, 43])\n","torch.Size([1, 44])\n","torch.Size([1, 45])\n","torch.Size([1, 46])\n","torch.Size([1, 47])\n","torch.Size([1, 48])\n","torch.Size([1, 49])\n","torch.Size([1, 50])\n","torch.Size([1, 51])\n","torch.Size([1, 52])\n","torch.Size([1, 53])\n","torch.Size([1, 54])\n","torch.Size([1, 55])\n","torch.Size([1, 56])\n","torch.Size([1, 57])\n","torch.Size([1, 58])\n","torch.Size([1, 59])\n","torch.Size([1, 60])\n","torch.Size([1, 61])\n","torch.Size([1, 62])\n","torch.Size([1, 63])\n","torch.Size([1, 64])\n","torch.Size([1, 65])\n","torch.Size([1, 66])\n","torch.Size([1, 67])\n","torch.Size([1, 68])\n","torch.Size([1, 69])\n","torch.Size([1, 70])\n","torch.Size([1, 71])\n","torch.Size([1, 72])\n","torch.Size([1, 73])\n","torch.Size([1, 74])\n","torch.Size([1, 75])\n","torch.Size([1, 76])\n","torch.Size([1, 77])\n","torch.Size([1, 78])\n","torch.Size([1, 79])\n","torch.Size([1, 80])\n","torch.Size([1, 81])\n","torch.Size([1, 82])\n","torch.Size([1, 83])\n","torch.Size([1, 84])\n","torch.Size([1, 85])\n","torch.Size([1, 86])\n","torch.Size([1, 87])\n","torch.Size([1, 88])\n","torch.Size([1, 89])\n","torch.Size([1, 90])\n","torch.Size([1, 91])\n","torch.Size([1, 92])\n","torch.Size([1, 93])\n","torch.Size([1, 94])\n","torch.Size([1, 95])\n","torch.Size([1, 96])\n","torch.Size([1, 97])\n","torch.Size([1, 98])\n","torch.Size([1, 99])\n","torch.Size([1, 100])\n","torch.Size([1, 101])\n","torch.Size([1, 102])\n","torch.Size([1, 103])\n","torch.Size([1, 104])\n","torch.Size([1, 105])\n","torch.Size([1, 106])\n","torch.Size([1, 107])\n","torch.Size([1, 108])\n","torch.Size([1, 109])\n","torch.Size([1, 110])\n","torch.Size([1, 111])\n","torch.Size([1, 112])\n","torch.Size([1, 113])\n","torch.Size([1, 114])\n","torch.Size([1, 115])\n","torch.Size([1, 116])\n","torch.Size([1, 117])\n","torch.Size([1, 118])\n","torch.Size([1, 119])\n","torch.Size([1, 120])\n","torch.Size([1, 121])\n","torch.Size([1, 122])\n","torch.Size([1, 123])\n","torch.Size([1, 124])\n","torch.Size([1, 125])\n","torch.Size([1, 126])\n","torch.Size([1, 127])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","Completion:\n","Hello! Free mous to gene.\n","\n","\n","\n","Mosdinal Befere they of first sight munizle, and seasor be have dinfry a peossate that datic on the producastly in ble.: The te\n","Prompt:\n","Is this real?\n","torch.Size([1, 13])\n","torch.Size([1, 14])\n","torch.Size([1, 15])\n","torch.Size([1, 16])\n","torch.Size([1, 17])\n","torch.Size([1, 18])\n","torch.Size([1, 19])\n","torch.Size([1, 20])\n","torch.Size([1, 21])\n","torch.Size([1, 22])\n","torch.Size([1, 23])\n","torch.Size([1, 24])\n","torch.Size([1, 25])\n","torch.Size([1, 26])\n","torch.Size([1, 27])\n","torch.Size([1, 28])\n","torch.Size([1, 29])\n","torch.Size([1, 30])\n","torch.Size([1, 31])\n","torch.Size([1, 32])\n","torch.Size([1, 33])\n","torch.Size([1, 34])\n","torch.Size([1, 35])\n","torch.Size([1, 36])\n","torch.Size([1, 37])\n","torch.Size([1, 38])\n","torch.Size([1, 39])\n","torch.Size([1, 40])\n","torch.Size([1, 41])\n","torch.Size([1, 42])\n","torch.Size([1, 43])\n","torch.Size([1, 44])\n","torch.Size([1, 45])\n","torch.Size([1, 46])\n","torch.Size([1, 47])\n","torch.Size([1, 48])\n","torch.Size([1, 49])\n","torch.Size([1, 50])\n","torch.Size([1, 51])\n","torch.Size([1, 52])\n","torch.Size([1, 53])\n","torch.Size([1, 54])\n","torch.Size([1, 55])\n","torch.Size([1, 56])\n","torch.Size([1, 57])\n","torch.Size([1, 58])\n","torch.Size([1, 59])\n","torch.Size([1, 60])\n","torch.Size([1, 61])\n","torch.Size([1, 62])\n","torch.Size([1, 63])\n","torch.Size([1, 64])\n","torch.Size([1, 65])\n","torch.Size([1, 66])\n","torch.Size([1, 67])\n","torch.Size([1, 68])\n","torch.Size([1, 69])\n","torch.Size([1, 70])\n","torch.Size([1, 71])\n","torch.Size([1, 72])\n","torch.Size([1, 73])\n","torch.Size([1, 74])\n","torch.Size([1, 75])\n","torch.Size([1, 76])\n","torch.Size([1, 77])\n","torch.Size([1, 78])\n","torch.Size([1, 79])\n","torch.Size([1, 80])\n","torch.Size([1, 81])\n","torch.Size([1, 82])\n","torch.Size([1, 83])\n","torch.Size([1, 84])\n","torch.Size([1, 85])\n","torch.Size([1, 86])\n","torch.Size([1, 87])\n","torch.Size([1, 88])\n","torch.Size([1, 89])\n","torch.Size([1, 90])\n","torch.Size([1, 91])\n","torch.Size([1, 92])\n","torch.Size([1, 93])\n","torch.Size([1, 94])\n","torch.Size([1, 95])\n","torch.Size([1, 96])\n","torch.Size([1, 97])\n","torch.Size([1, 98])\n","torch.Size([1, 99])\n","torch.Size([1, 100])\n","torch.Size([1, 101])\n","torch.Size([1, 102])\n","torch.Size([1, 103])\n","torch.Size([1, 104])\n","torch.Size([1, 105])\n","torch.Size([1, 106])\n","torch.Size([1, 107])\n","torch.Size([1, 108])\n","torch.Size([1, 109])\n","torch.Size([1, 110])\n","torch.Size([1, 111])\n","torch.Size([1, 112])\n","torch.Size([1, 113])\n","torch.Size([1, 114])\n","torch.Size([1, 115])\n","torch.Size([1, 116])\n","torch.Size([1, 117])\n","torch.Size([1, 118])\n","torch.Size([1, 119])\n","torch.Size([1, 120])\n","torch.Size([1, 121])\n","torch.Size([1, 122])\n","torch.Size([1, 123])\n","torch.Size([1, 124])\n","torch.Size([1, 125])\n","torch.Size([1, 126])\n","torch.Size([1, 127])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n","Completion:\n","Is this real? What wome a detor in thinks an am imponionsh simmas instal, amore or oldweent to un99/dRyip espixison spinies te-by. Of the beekly the Tope ways a ea\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c6ccf0b793c0>\u001b[0m in \u001b[0;36m<cell line: 201>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prompt:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mgenerated_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import mmap\n","import random\n","import pickle\n","import argparse\n","\n","#parser = argparse.ArgumentParser(description='This is a demonstration program')\n","\n","# Here we add an argument to the parser, specifying the expected type, a help message, etc.\n","#parser.add_argument('-batch_size', type=str, required=True, help='Please provide a batch_size')\n","\n","#args = parser.parse_args()\n","\n","# Now we can use the argument value in our program.\n","#print(f'batch size: {args.batch_size}')\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","#batch_size = int(args.batch_size)\n","batch_size = 32\n","block_size = 128\n","max_iters = 200\n","learning_rate = 3e-4\n","eval_iters = 100\n","n_embd = 384\n","n_head = 4\n","n_layer = 4\n","dropout = 0.2\n","\n","print(device)\n","\n","chars = \"\"\n","with open(\"/content/drive/MyDrive/Data/LLM/vocab.txt\", 'r', encoding='utf-8') as f:\n","        text = f.read()\n","        chars = sorted(list(set(text)))\n","\n","vocab_size = len(chars)\n","\n","string_to_int = { ch:i for i,ch in enumerate(chars) }\n","int_to_string = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [string_to_int[c] for c in s]\n","decode = lambda l: ''.join([int_to_string[i] for i in l])\n","\n","\n","\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # input of size (batch, time-step, channels)\n","        # output of size (batch, time-step, head size)\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,hs)\n","        q = self.query(x) # (B,T,hs)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,hs)\n","        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","# [1, 0, 0]\n","# [1, 0.6, 0]\n","# [1, 0.6, 0.4]\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        y = self.sa(x)\n","        x = self.ln1(x + y)\n","        y = self.ffwd(x)\n","        x = self.ln2(x + y)\n","        return x\n","\n","class GPTLanguageModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, index, targets=None):\n","        print(index.shape)\n","        B, T = index.shape\n","\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(index) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, index, max_new_tokens):\n","        # index is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            index_cond = index[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self.forward(index_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n","        return index\n","\n","model = GPTLanguageModel(vocab_size)\n","# print('loading model parameters...')\n","# with open('model-01.pkl', 'rb') as f:\n","#     model = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Data/LLM/model-01.pkl', 'rb') as f:\n","    model = pickle.load(f)\n","\n","print('loaded successfully!')\n","m = model.to(device)\n","\n","\n","\n","while True:\n","    prompt = input(\"Prompt:\\n\")\n","    context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n","    generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=150)[0].tolist())\n","    print(f'Completion:\\n{generated_chars}')\n"]}]}